{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer/tokenizer_config.json',\n",
       " './roberta_tokenizer/special_tokens_map.json',\n",
       " './roberta_tokenizer/vocab.txt',\n",
       " './roberta_tokenizer/added_tokens.json',\n",
       " './roberta_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##竅': 18043,\n",
       " '要': 6206,\n",
       " 'january': 9768,\n",
       " '黎': 7944,\n",
       " '##rmb': 10894,\n",
       " '鎂': 7110,\n",
       " '塘': 1851,\n",
       " '昭': 3220,\n",
       " '##酒': 20040,\n",
       " '##飢': 20666,\n",
       " '##龛': 21046,\n",
       " '##訝': 19309,\n",
       " '夜': 1915,\n",
       " '##潇': 17102,\n",
       " '##襟': 19256,\n",
       " '188': 9460,\n",
       " '##骅': 20792,\n",
       " '##蕴': 19000,\n",
       " 'washington': 12262,\n",
       " '##锁': 20276,\n",
       " 'ィ': 590,\n",
       " 'amazon': 8954,\n",
       " '7866': 12081,\n",
       " '瘩': 4609,\n",
       " 'なとの': 10435,\n",
       " '##ware': 10534,\n",
       " '013': 13034,\n",
       " 'pet': 10495,\n",
       " '##凪': 14187,\n",
       " '##なり': 11592,\n",
       " '##board': 9472,\n",
       " '⦿': 509,\n",
       " '燴': 4251,\n",
       " '諦': 6320,\n",
       " '##忧': 15626,\n",
       " 'っています': 12202,\n",
       " '##报': 15902,\n",
       " '樯': 3568,\n",
       " '##插': 16048,\n",
       " '##斜': 16219,\n",
       " '##枋': 16414,\n",
       " '##さ': 11647,\n",
       " '262': 10975,\n",
       " '##谔': 19516,\n",
       " '##躇': 19766,\n",
       " '⒊': 422,\n",
       " '##残': 16712,\n",
       " '##笠': 18072,\n",
       " '##峦': 15344,\n",
       " '##杀': 16381,\n",
       " '燿': 4254,\n",
       " '##偻': 14040,\n",
       " 'や': 573,\n",
       " '？': 8043,\n",
       " '##冒': 14145,\n",
       " '##蒞': 18944,\n",
       " '##磺': 17898,\n",
       " '274': 11482,\n",
       " '##fet': 12089,\n",
       " '舍': 5650,\n",
       " '疥': 4553,\n",
       " '剤': 1194,\n",
       " '##犢': 17360,\n",
       " '限': 7361,\n",
       " '1983': 8715,\n",
       " '##ald': 13041,\n",
       " 'к': 242,\n",
       " '景': 3250,\n",
       " '▲topnov': 10502,\n",
       " 'ｅ': 8055,\n",
       " 'day': 8542,\n",
       " '##咄': 14523,\n",
       " '##馥': 20734,\n",
       " 'cry': 10485,\n",
       " '50g': 12233,\n",
       " '♥yoyo♥': 8948,\n",
       " '##mah': 10394,\n",
       " '[unused58]': 58,\n",
       " '##卦': 14365,\n",
       " '##嘞': 14717,\n",
       " '##欽': 16677,\n",
       " '贏': 6560,\n",
       " '##折': 15892,\n",
       " '##懲': 15809,\n",
       " '├': 436,\n",
       " 'sars': 12390,\n",
       " '##now': 11371,\n",
       " '凹': 1138,\n",
       " '##惰': 15737,\n",
       " '##阴': 20403,\n",
       " '##韶': 20568,\n",
       " '囱': 1738,\n",
       " '學': 2119,\n",
       " '授': 2956,\n",
       " '冬': 1100,\n",
       " '266': 9674,\n",
       " '龜': 7990,\n",
       " '##クok': 13040,\n",
       " '║': 439,\n",
       " '豎': 6492,\n",
       " '##圆': 14806,\n",
       " '囔': 1721,\n",
       " '##vd': 10754,\n",
       " '##巫': 15401,\n",
       " '##消': 16924,\n",
       " '##雄': 20470,\n",
       " '##崴': 15368,\n",
       " 'ᅩ': 311,\n",
       " 'f': 148,\n",
       " '##函': 14198,\n",
       " '絵': 5189,\n",
       " '##莠': 18864,\n",
       " '##牛': 17338,\n",
       " '153': 9677,\n",
       " 'shadow': 12378,\n",
       " '##觐': 19290,\n",
       " '##辮': 19856,\n",
       " '##錮': 20153,\n",
       " '##総': 18274,\n",
       " '##涎': 16927,\n",
       " '姣': 2004,\n",
       " 'pm': 8397,\n",
       " '爲': 4264,\n",
       " '荔': 5775,\n",
       " '堯': 1839,\n",
       " '##盂': 17712,\n",
       " '##豹': 19558,\n",
       " '羿': 5418,\n",
       " '虹': 6004,\n",
       " '零': 7439,\n",
       " 'てした': 11331,\n",
       " '##恨': 15673,\n",
       " '##玺': 17446,\n",
       " '帼': 2383,\n",
       " '##咁': 14521,\n",
       " '蹩': 6699,\n",
       " '封': 2196,\n",
       " 'vs': 8349,\n",
       " 'cross': 12209,\n",
       " '►': 467,\n",
       " '5kg': 13183,\n",
       " '##攘': 16163,\n",
       " '##蟄': 19149,\n",
       " '##ルフ': 12968,\n",
       " '##vin': 10454,\n",
       " '施': 3177,\n",
       " '382': 12163,\n",
       " '##醣': 20070,\n",
       " '##ㄎ': 13712,\n",
       " '沂': 3752,\n",
       " '訕': 6247,\n",
       " '##ウ': 12891,\n",
       " '##趵': 19698,\n",
       " '##贺': 19647,\n",
       " '##mll': 10682,\n",
       " '##医': 14335,\n",
       " '縈': 5235,\n",
       " '滁': 3991,\n",
       " '峒': 2282,\n",
       " '龔': 7985,\n",
       " 'black': 9296,\n",
       " '挫': 2919,\n",
       " '##蹦': 19755,\n",
       " '##¹': 13357,\n",
       " '##幟': 15449,\n",
       " '##焦': 17250,\n",
       " '禀': 4880,\n",
       " '##nce': 9056,\n",
       " '##侖': 13952,\n",
       " '##篑': 18122,\n",
       " '##纳': 18344,\n",
       " '劭': 1224,\n",
       " '##詆': 19322,\n",
       " '##鏤': 20187,\n",
       " '##飓': 20658,\n",
       " '350': 8612,\n",
       " 'ofweek': 10140,\n",
       " '##獄': 17409,\n",
       " '珺': 4411,\n",
       " 'lo': 12264,\n",
       " '##図': 14797,\n",
       " '##⒊': 13573,\n",
       " '##休': 13885,\n",
       " '##样': 16473,\n",
       " '隨': 7401,\n",
       " '##啉': 14613,\n",
       " '##谆': 19504,\n",
       " '肿': 5514,\n",
       " '稀': 4921,\n",
       " '椁': 3487,\n",
       " '##瓏': 17533,\n",
       " '##nnis': 13004,\n",
       " '乜': 735,\n",
       " '##贈': 19614,\n",
       " '##腺': 18650,\n",
       " '##酯': 20051,\n",
       " '##撬': 16120,\n",
       " '##族': 16241,\n",
       " '茏': 5750,\n",
       " '##╱': 13595,\n",
       " '貞': 6510,\n",
       " '哮': 1527,\n",
       " '##求': 16781,\n",
       " '辱': 6802,\n",
       " '藩': 5974,\n",
       " '##伍': 13881,\n",
       " '觅': 6227,\n",
       " '恪': 2618,\n",
       " '##囊': 14775,\n",
       " '凯': 1132,\n",
       " '##濕': 17143,\n",
       " '##臍': 18681,\n",
       " '##情': 15715,\n",
       " '##閣': 20341,\n",
       " '倆': 941,\n",
       " '##莆': 18855,\n",
       " '##沣': 16825,\n",
       " '##灌': 17175,\n",
       " '##僵': 14075,\n",
       " '鳩': 7853,\n",
       " 'april': 9557,\n",
       " '##one': 9021,\n",
       " 'tpp': 8855,\n",
       " '3ce': 10183,\n",
       " '賦': 6548,\n",
       " 'from': 8670,\n",
       " '##king': 9740,\n",
       " '谏': 6454,\n",
       " '##14': 8717,\n",
       " '##rss': 12281,\n",
       " '##mine': 12723,\n",
       " '″': 346,\n",
       " '##苓': 18782,\n",
       " '丛': 690,\n",
       " '綏': 5193,\n",
       " '##虬': 19059,\n",
       " '沾': 3783,\n",
       " '詭': 6279,\n",
       " '贸': 6588,\n",
       " '颞': 7584,\n",
       " '凑': 1122,\n",
       " 'wi': 8541,\n",
       " '##繇': 18306,\n",
       " '噻': 1699,\n",
       " 'blogger': 9742,\n",
       " '##啓': 14616,\n",
       " '##然': 17254,\n",
       " 'я': 259,\n",
       " '##飛': 20663,\n",
       " '##a5': 12540,\n",
       " '##疹': 17619,\n",
       " '##磕': 17890,\n",
       " '碚': 4815,\n",
       " '洲': 3828,\n",
       " '##颔': 20633,\n",
       " 'mount': 12881,\n",
       " '％': 8017,\n",
       " '﹂': 7997,\n",
       " '##縛': 18293,\n",
       " '皚': 4648,\n",
       " '##悚': 15696,\n",
       " '覆': 6208,\n",
       " '##货': 19630,\n",
       " '亵': 781,\n",
       " '##on': 8224,\n",
       " 'series': 10976,\n",
       " '##锥': 20295,\n",
       " '渗': 3935,\n",
       " '##榕': 16582,\n",
       " '肃': 5484,\n",
       " 'live': 8582,\n",
       " '246': 11003,\n",
       " 'sina': 9606,\n",
       " '##+': 13327,\n",
       " '根': 3418,\n",
       " 'rate': 12597,\n",
       " '攒': 3104,\n",
       " '飨': 7610,\n",
       " '##足': 19696,\n",
       " '蟑': 6096,\n",
       " '##97': 9410,\n",
       " '##石': 17824,\n",
       " '##鸾': 20952,\n",
       " '栉': 3405,\n",
       " '胎': 5522,\n",
       " '预': 7564,\n",
       " '##伴': 13902,\n",
       " '[unused28]': 28,\n",
       " '沱': 3776,\n",
       " '##之': 13779,\n",
       " '##撑': 16110,\n",
       " '##坦': 14845,\n",
       " 'gomaji': 9576,\n",
       " '##荡': 18839,\n",
       " '##昱': 16279,\n",
       " '藤': 5972,\n",
       " '##室': 15204,\n",
       " '##罢': 18444,\n",
       " 'cf': 9191,\n",
       " '钯': 7176,\n",
       " '##霎': 20510,\n",
       " '粕': 5109,\n",
       " '蜿': 6065,\n",
       " '##鍰': 20163,\n",
       " '1997': 8387,\n",
       " '臓': 5625,\n",
       " '247': 11128,\n",
       " '##蓮': 18966,\n",
       " '區': 1281,\n",
       " '##敦': 16199,\n",
       " 'no1': 11448,\n",
       " '攙': 3107,\n",
       " '##伎': 13882,\n",
       " '##潟': 17111,\n",
       " '礁': 4842,\n",
       " '茉': 5748,\n",
       " '##咣': 14539,\n",
       " '菁': 5821,\n",
       " '蛔': 6031,\n",
       " '##嗚': 14684,\n",
       " '勢': 1248,\n",
       " '述': 6835,\n",
       " '##用': 17557,\n",
       " '##檬': 16654,\n",
       " '##sm': 10244,\n",
       " '恥': 2615,\n",
       " '##响': 14567,\n",
       " '##睇': 17767,\n",
       " '##怔': 15642,\n",
       " '##麺': 20993,\n",
       " '##ス': 8551,\n",
       " '##疊': 17597,\n",
       " '##暮': 16329,\n",
       " 'gold': 10628,\n",
       " '喺': 1615,\n",
       " '注': 3800,\n",
       " '3000': 8283,\n",
       " '奠': 1950,\n",
       " '##lton': 10377,\n",
       " 'ka': 11306,\n",
       " 'ャ': 629,\n",
       " '##厨': 14394,\n",
       " '条': 3340,\n",
       " '##鈦': 20106,\n",
       " '逃': 6845,\n",
       " 'lamigo': 12536,\n",
       " '##晩': 16305,\n",
       " '##瑕': 17499,\n",
       " '##硅': 17852,\n",
       " '宕': 2133,\n",
       " '蹺': 6705,\n",
       " 'security': 11547,\n",
       " 'ah': 10785,\n",
       " '##崭': 15367,\n",
       " '##鑣': 20200,\n",
       " '褚': 6190,\n",
       " '##est': 10414,\n",
       " '##cket': 10811,\n",
       " '##儲': 14090,\n",
       " '湟': 3962,\n",
       " 'gov': 9514,\n",
       " '##雹': 20498,\n",
       " '##鹭': 20972,\n",
       " 'homemesh': 9167,\n",
       " '##橼': 16645,\n",
       " '神': 4868,\n",
       " '##于': 13811,\n",
       " '##勳': 14308,\n",
       " '##類': 20603,\n",
       " '##嫻': 15135,\n",
       " '##ek': 9534,\n",
       " 'les': 10133,\n",
       " '##丙': 13745,\n",
       " '##肱': 18565,\n",
       " '##￥': 21123,\n",
       " '##馏': 20728,\n",
       " '欸': 3618,\n",
       " '▲topjul': 10572,\n",
       " '##箴': 18114,\n",
       " '虜': 5996,\n",
       " '岐': 2262,\n",
       " '利': 1164,\n",
       " '汴': 3745,\n",
       " '质': 6574,\n",
       " '##ᆸ': 13485,\n",
       " '##椹': 16554,\n",
       " '##ｐ': 10748,\n",
       " '##菜': 18888,\n",
       " '##鯰': 20867,\n",
       " '##従': 15588,\n",
       " '浆': 3841,\n",
       " '霭': 7461,\n",
       " '喜': 1599,\n",
       " '##2015': 8600,\n",
       " '左': 2340,\n",
       " '##躪': 19772,\n",
       " '烈': 4164,\n",
       " '##雀': 20468,\n",
       " 'とは': 11556,\n",
       " 'vga': 13140,\n",
       " '124': 9377,\n",
       " '##揩': 16054,\n",
       " '##鬥': 20841,\n",
       " '百': 4636,\n",
       " '##缠': 18419,\n",
       " '胯': 5535,\n",
       " '閲': 7289,\n",
       " '620': 11078,\n",
       " 'download': 9461,\n",
       " '弛': 2474,\n",
       " '撓': 3055,\n",
       " '鳄': 7843,\n",
       " '5c': 13025,\n",
       " '##stry': 11001,\n",
       " '##陰': 20431,\n",
       " '##嫂': 15121,\n",
       " '456': 12570,\n",
       " '奖': 1946,\n",
       " '##肛': 18554,\n",
       " '##釘': 20091,\n",
       " '伤': 839,\n",
       " 'years': 8753,\n",
       " '##膺': 18670,\n",
       " '##蘼': 19043,\n",
       " '檳': 3599,\n",
       " '##３': 9089,\n",
       " 'ebd': 8539,\n",
       " 'size': 9571,\n",
       " '##メ': 10597,\n",
       " '＊': 8022,\n",
       " '##呈': 14496,\n",
       " '##sa': 8606,\n",
       " 'tiffany': 11509,\n",
       " 'gundam': 12523,\n",
       " '耸': 5458,\n",
       " 'akb48': 10521,\n",
       " '##宣': 15203,\n",
       " '##ρ': 13394,\n",
       " 'win7': 8586,\n",
       " '束': 3338,\n",
       " '##同': 14455,\n",
       " '氟': 3703,\n",
       " '苏': 5722,\n",
       " '##晴': 16309,\n",
       " '##祥': 17929,\n",
       " '##棋': 16527,\n",
       " '箋': 5044,\n",
       " '[unused86]': 86,\n",
       " '##尚': 15270,\n",
       " '##湘': 17017,\n",
       " '踪': 6679,\n",
       " '##诧': 19480,\n",
       " '##唆': 14591,\n",
       " '﹁': 7996,\n",
       " 'message': 12231,\n",
       " 'restaurant': 10637,\n",
       " '6500': 11247,\n",
       " '##嚷': 14771,\n",
       " '茜': 5752,\n",
       " 'ぃ': 535,\n",
       " '##潧': 17115,\n",
       " '镁': 7250,\n",
       " '咙': 1479,\n",
       " '269': 11023,\n",
       " 'wong': 11615,\n",
       " '杭': 3343,\n",
       " 'jan': 9213,\n",
       " '##締': 18281,\n",
       " '##旧': 16248,\n",
       " '碗': 4813,\n",
       " 'edited': 10304,\n",
       " '尸': 2221,\n",
       " '燙': 4243,\n",
       " '##餚': 20684,\n",
       " '噙': 1685,\n",
       " '##泵': 16865,\n",
       " '377': 13222,\n",
       " '蛾': 6042,\n",
       " 'skip': 8973,\n",
       " '333': 10745,\n",
       " 'sb': 12359,\n",
       " '蚕': 6014,\n",
       " '抬': 2848,\n",
       " '##攥': 16171,\n",
       " '##诽': 19496,\n",
       " '戶': 2786,\n",
       " 'email': 8307,\n",
       " '32': 8211,\n",
       " '茨': 5754,\n",
       " '##岑': 15320,\n",
       " '##忌': 15612,\n",
       " '##貽': 19586,\n",
       " '克': 1046,\n",
       " '彻': 2515,\n",
       " '428': 11351,\n",
       " '##佰': 13937,\n",
       " '##峒': 15339,\n",
       " 'part': 9124,\n",
       " '##对': 15247,\n",
       " '##桠': 16494,\n",
       " '##閏': 20333,\n",
       " '386': 12303,\n",
       " '南': 1298,\n",
       " '##麸': 20992,\n",
       " '潭': 4059,\n",
       " '##弩': 15537,\n",
       " '婚': 2042,\n",
       " '張': 2484,\n",
       " '000': 8241,\n",
       " '##来': 16398,\n",
       " '##by': 8684,\n",
       " '##ツ': 10419,\n",
       " 'ᅳ': 318,\n",
       " '##泡': 16853,\n",
       " '芷': 5711,\n",
       " '##虑': 19048,\n",
       " '##hin': 13098,\n",
       " '挨': 2917,\n",
       " '##設': 19314,\n",
       " '##诟': 19472,\n",
       " '妳': 1986,\n",
       " '徉': 2524,\n",
       " '##玻': 17447,\n",
       " '蚩': 6019,\n",
       " '##▪': 13607,\n",
       " '##囝': 14782,\n",
       " '应': 2418,\n",
       " '##冉': 14140,\n",
       " '丑': 682,\n",
       " '##秀': 17956,\n",
       " '##蒹': 18950,\n",
       " '淇': 3899,\n",
       " '叻': 1387,\n",
       " '##89': 9402,\n",
       " 'content': 9432,\n",
       " '謾': 6347,\n",
       " 'details': 13227,\n",
       " '##鎌': 20169,\n",
       " '##ten': 11598,\n",
       " '##载': 19827,\n",
       " 'jose': 12606,\n",
       " '挾': 2925,\n",
       " '地': 1765,\n",
       " '搭': 3022,\n",
       " '月': 3299,\n",
       " '氮': 3713,\n",
       " '湄': 3956,\n",
       " '睹': 4726,\n",
       " '补': 6133,\n",
       " 'eclipse': 11752,\n",
       " '##測': 17004,\n",
       " '佐': 858,\n",
       " '##理': 17472,\n",
       " '##輝': 19797,\n",
       " '##鐮': 20190,\n",
       " '盒': 4665,\n",
       " '##娛': 15081,\n",
       " '##挑': 15961,\n",
       " '瑛': 4446,\n",
       " '撸': 3070,\n",
       " '##衢': 19188,\n",
       " '##豺': 19559,\n",
       " '船': 5670,\n",
       " '慧': 2716,\n",
       " '３': 8031,\n",
       " '##啪': 14627,\n",
       " '##樸': 16628,\n",
       " '践': 6664,\n",
       " '㊣': 668,\n",
       " 'tue': 8483,\n",
       " '##脅': 18602,\n",
       " '##先': 14101,\n",
       " '##贮': 19637,\n",
       " '怠': 2591,\n",
       " '京': 776,\n",
       " '裘': 6169,\n",
       " '处': 1905,\n",
       " '##﹙': 21064,\n",
       " '雄': 7413,\n",
       " '睑': 4713,\n",
       " '聘': 5470,\n",
       " '弹': 2486,\n",
       " '鸯': 7891,\n",
       " '泓': 3790,\n",
       " '衆': 6120,\n",
       " 'wang': 9660,\n",
       " '##over': 11436,\n",
       " '##ㄟ': 13718,\n",
       " '##还': 19877,\n",
       " '酋': 6979,\n",
       " '##异': 15517,\n",
       " '##萨': 18912,\n",
       " '##镇': 20309,\n",
       " '芒': 5695,\n",
       " '芾': 5716,\n",
       " '茹': 5765,\n",
       " 'erp': 9529,\n",
       " '##杯': 16401,\n",
       " '##蓋': 18958,\n",
       " '墩': 1875,\n",
       " '鞦': 7497,\n",
       " '劣': 1219,\n",
       " '##热': 17235,\n",
       " '##铁': 20245,\n",
       " '##能': 18600,\n",
       " '##悲': 15707,\n",
       " '##狞': 17377,\n",
       " '膚': 5604,\n",
       " '涞': 3877,\n",
       " '能': 5543,\n",
       " 'end': 9931,\n",
       " '182': 9952,\n",
       " '哋': 1508,\n",
       " '丢': 696,\n",
       " '搗': 3016,\n",
       " '猙': 4337,\n",
       " '407': 12458,\n",
       " '##煒': 17261,\n",
       " '##峨': 15345,\n",
       " 'vive': 10167,\n",
       " '##のお': 10217,\n",
       " '##lot': 10580,\n",
       " '˙': 207,\n",
       " '##篡': 18127,\n",
       " '栈': 3404,\n",
       " '葭': 5874,\n",
       " '##ong': 9142,\n",
       " '勞': 1246,\n",
       " '40': 8164,\n",
       " '##house': 9937,\n",
       " 'vera': 9378,\n",
       " '##々': 13648,\n",
       " '亂': 748,\n",
       " '##們': 14004,\n",
       " '##仁': 13842,\n",
       " '##憔': 15789,\n",
       " '##裏': 19223,\n",
       " '##貪': 19574,\n",
       " '##乩': 13799,\n",
       " '##転': 19785,\n",
       " '##鲑': 20886,\n",
       " '##髮': 20830,\n",
       " '浏': 3846,\n",
       " '##矇': 17810,\n",
       " 'ㄌ': 652,\n",
       " '雑': 7423,\n",
       " '##賓': 19597,\n",
       " '##跋': 19705,\n",
       " '##莢': 18865,\n",
       " '##佞': 13927,\n",
       " '##氖': 16756,\n",
       " '绝': 5318,\n",
       " 'internet': 8484,\n",
       " '##屬': 15310,\n",
       " '##mini': 12391,\n",
       " '癒': 4618,\n",
       " '耳': 5455,\n",
       " '霞': 7459,\n",
       " '##盛': 17727,\n",
       " '##▬': 13609,\n",
       " 'public': 10672,\n",
       " '##あります': 10700,\n",
       " '疮': 4555,\n",
       " '脛': 5559,\n",
       " '##は': 8319,\n",
       " 'item': 9279,\n",
       " '忿': 2576,\n",
       " '探': 2968,\n",
       " 'mar': 9118,\n",
       " '##ren': 9489,\n",
       " '##她': 15018,\n",
       " '##尸': 15278,\n",
       " '##鯉': 20862,\n",
       " '情': 2658,\n",
       " '裤': 6175,\n",
       " '##爺': 17327,\n",
       " '##曰': 16345,\n",
       " '##矍': 17811,\n",
       " '##ment': 8631,\n",
       " 'wing': 12349,\n",
       " '喉': 1590,\n",
       " '2900': 13044,\n",
       " '##池': 16794,\n",
       " '##剁': 14233,\n",
       " 'ヽ': 646,\n",
       " '洼': 3834,\n",
       " '##rap': 11893,\n",
       " 'ips': 10511,\n",
       " '##淫': 16972,\n",
       " 'g9': 11157,\n",
       " '##滁': 17048,\n",
       " '虽': 6006,\n",
       " 'ti': 9654,\n",
       " '桠': 3437,\n",
       " '##詫': 19334,\n",
       " '凳': 1135,\n",
       " '疲': 4558,\n",
       " '##︰': 21049,\n",
       " '瀑': 4105,\n",
       " 'm3': 9305,\n",
       " '##＾': 21092,\n",
       " '[unused63]': 63,\n",
       " '拭': 2887,\n",
       " '##san': 10978,\n",
       " '##崽': 15369,\n",
       " '##亩': 13831,\n",
       " '瞓': 4738,\n",
       " '3～4': 12907,\n",
       " '##梵': 16521,\n",
       " '聆': 5463,\n",
       " '##抨': 15903,\n",
       " '坏': 1776,\n",
       " '錮': 7096,\n",
       " '姗': 2000,\n",
       " '##ors': 10903,\n",
       " '830': 12428,\n",
       " '##2f': 12971,\n",
       " '##慈': 15762,\n",
       " '昴': 3223,\n",
       " '##谍': 19509,\n",
       " '##連': 19922,\n",
       " '187': 10058,\n",
       " '##ns': 8727,\n",
       " '##相': 17742,\n",
       " '##ugh': 12667,\n",
       " '5m': 11483,\n",
       " '痪': 4586,\n",
       " '汛': 3733,\n",
       " '##币': 15412,\n",
       " '剷': 1202,\n",
       " '##喆': 14645,\n",
       " '##农': 14150,\n",
       " 'east': 10990,\n",
       " 'song': 11502,\n",
       " '##２': 8929,\n",
       " '訛': 6251,\n",
       " '淳': 3919,\n",
       " '007': 9722,\n",
       " '##紐': 18210,\n",
       " '##壮': 14953,\n",
       " '扰': 2817,\n",
       " '尤': 2215,\n",
       " '胆': 5519,\n",
       " '荀': 5767,\n",
       " '藐': 5967,\n",
       " '1882': 13273,\n",
       " '漏': 4026,\n",
       " '##cm': 8341,\n",
       " 'b1': 9338,\n",
       " '##51': 9216,\n",
       " '##塞': 14910,\n",
       " '##妖': 15030,\n",
       " '惨': 2673,\n",
       " 'wordpress': 8627,\n",
       " 'uniqlo': 11905,\n",
       " 'サ': 603,\n",
       " '##汀': 16779,\n",
       " '赐': 6606,\n",
       " '惡': 2670,\n",
       " '##‰': 13503,\n",
       " '##眶': 17759,\n",
       " '##耕': 18506,\n",
       " 'cms': 11539,\n",
       " '##膨': 18667,\n",
       " '棕': 3473,\n",
       " '##za': 9283,\n",
       " '↘': 374,\n",
       " '阉': 7329,\n",
       " '##祖': 17919,\n",
       " '1913': 10432,\n",
       " 'living': 12113,\n",
       " 'ces': 9818,\n",
       " 'ryan': 11536,\n",
       " '##蓬': 18965,\n",
       " '##叮': 14433,\n",
       " '##鲛': 20887,\n",
       " '##郑': 20005,\n",
       " '##芽': 18772,\n",
       " 'mit': 9315,\n",
       " 'ui': 8840,\n",
       " '癞': 4621,\n",
       " 'l': 154,\n",
       " '癸': 4631,\n",
       " '铛': 7197,\n",
       " 'sea': 11007,\n",
       " '钎': 7154,\n",
       " '##out': 9408,\n",
       " '桜': 3436,\n",
       " '##嬴': 15145,\n",
       " 'cream': 11494,\n",
       " '##愉': 15747,\n",
       " '##呐': 14500,\n",
       " '##謝': 19399,\n",
       " '韧': 7505,\n",
       " '签': 5041,\n",
       " 'via': 10003,\n",
       " 'してくたさい♪この': 12151,\n",
       " '郅': 6945,\n",
       " '##墀': 14918,\n",
       " '荫': 5789,\n",
       " '##rman': 11386,\n",
       " 'rc': 11746,\n",
       " '##仄': 13844,\n",
       " '##匆': 14317,\n",
       " '泸': 3810,\n",
       " '289': 11520,\n",
       " '##闫': 20364,\n",
       " 'long': 10037,\n",
       " '健': 978,\n",
       " '琛': 4422,\n",
       " '##际': 20411,\n",
       " 'spa': 8509,\n",
       " '⑤': 409,\n",
       " 'frank': 10379,\n",
       " 'than': 13158,\n",
       " '康': 2434,\n",
       " 'eric': 9836,\n",
       " 'tags': 9214,\n",
       " '攣': 3112,\n",
       " '娩': 2030,\n",
       " '1885': 13124,\n",
       " '偽': 984,\n",
       " '峋': 2281,\n",
       " '##侶': 13967,\n",
       " '钜': 7161,\n",
       " '##器': 14747,\n",
       " '纯': 5283,\n",
       " '訳': 6259,\n",
       " '##崖': 15361,\n",
       " 'φ': 229,\n",
       " '##嫣': 15130,\n",
       " '##勢': 14305,\n",
       " '##椽': 16555,\n",
       " '褫': 6193,\n",
       " '并': 2400,\n",
       " '了': 749,\n",
       " '##猜': 17396,\n",
       " 'copyright': 9521,\n",
       " '##焚': 17247,\n",
       " '##襁': 19254,\n",
       " '##蔔': 18971,\n",
       " '##驿': 20788,\n",
       " 'brian': 11754,\n",
       " '埔': 1815,\n",
       " '##惬': 15733,\n",
       " '##幀': 15442,\n",
       " '##翻': 18493,\n",
       " '懒': 2750,\n",
       " '##祗': 17920,\n",
       " '悸': 2654,\n",
       " '##すか': 11805,\n",
       " '##鬟': 20838,\n",
       " '##溧': 17039,\n",
       " 'λ': 219,\n",
       " '59': 8257,\n",
       " '##统': 18377,\n",
       " '##懇': 15801,\n",
       " '##繁': 18303,\n",
       " '##雙': 20484,\n",
       " '闯': 7310,\n",
       " '##83': 9559,\n",
       " '##柜': 16442,\n",
       " '##绫': 18386,\n",
       " '报': 2845,\n",
       " 'せ': 549,\n",
       " '痨': 4585,\n",
       " '绪': 5328,\n",
       " 'starbucks': 12919,\n",
       " '##∕': 13530,\n",
       " 'buff': 11321,\n",
       " '芡': 5699,\n",
       " 'てきちゃいます': 12136,\n",
       " '##與': 18702,\n",
       " '壘': 1888,\n",
       " '##皴': 17709,\n",
       " '##釜': 20092,\n",
       " '##钉': 20209,\n",
       " '濤': 4092,\n",
       " '┌': 434,\n",
       " '变': 1359,\n",
       " '抉': 2827,\n",
       " '一': 671,\n",
       " '蝠': 6075,\n",
       " '诡': 6417,\n",
       " '顛': 7545,\n",
       " '玟': 4378,\n",
       " '浴': 3861,\n",
       " '##五': 13815,\n",
       " '##冬': 14157,\n",
       " '攪': 3115,\n",
       " '事': 752,\n",
       " '塚': 1852,\n",
       " '磋': 4831,\n",
       " '谋': 6450,\n",
       " '囚': 1723,\n",
       " '均': 1772,\n",
       " '鬃': 7776,\n",
       " '##赧': 19678,\n",
       " '709': 12118,\n",
       " '##斓': 16215,\n",
       " 'fluke62max': 10946,\n",
       " '##瘁': 17653,\n",
       " 'camera': 11519,\n",
       " 'rmvb': 9365,\n",
       " 'works': 13112,\n",
       " '##無': 17249,\n",
       " '##《': 13652,\n",
       " '##鹂': 20954,\n",
       " 'tan': 12886,\n",
       " 'o': 157,\n",
       " '##膏': 18658,\n",
       " '##豐': 19550,\n",
       " '哈': 1506,\n",
       " '##纱': 18342,\n",
       " '##in': 8277,\n",
       " '##闵': 20371,\n",
       " 'pci': 10773,\n",
       " '##轨': 19815,\n",
       " '##午': 14343,\n",
       " 'fun': 9575,\n",
       " 'lock': 9189,\n",
       " '们': 812,\n",
       " '刚': 1157,\n",
       " '裟': 6173,\n",
       " '##aw': 10922,\n",
       " '##hone': 10271,\n",
       " 'q3': 10982,\n",
       " '##zone': 11004,\n",
       " 'van': 9933,\n",
       " '##fo': 10261,\n",
       " '帧': 2373,\n",
       " '鬓': 7779,\n",
       " 'california': 13293,\n",
       " '##、': 13645,\n",
       " '##蹲': 19759,\n",
       " '##繹': 18318,\n",
       " '##﹚': 21065,\n",
       " '##偃': 14025,\n",
       " '##懊': 15804,\n",
       " '##壘': 14945,\n",
       " '##赤': 19676,\n",
       " '轨': 6758,\n",
       " '鯖': 7807,\n",
       " '##watch': 9907,\n",
       " '直': 4684,\n",
       " '炼': 4159,\n",
       " '迄': 6812,\n",
       " '##獐': 17412,\n",
       " '##炙': 17204,\n",
       " '##訳': 19316,\n",
       " '##雅': 20471,\n",
       " '##搭': 16079,\n",
       " '１４': 12164,\n",
       " '##璁': 17517,\n",
       " '##汨': 16797,\n",
       " '##昙': 16271,\n",
       " 'ル': 637,\n",
       " '{': 169,\n",
       " '點': 7953,\n",
       " '##玷': 17445,\n",
       " '##翟': 18484,\n",
       " '琨': 4426,\n",
       " '##45': 9039,\n",
       " '##qq': 10624,\n",
       " '遽': 6911,\n",
       " '##呋': 14498,\n",
       " '成': 2768,\n",
       " '粲': 5119,\n",
       " '萬': 5857,\n",
       " '417': 13038,\n",
       " '##仟': 13863,\n",
       " '谙': 6463,\n",
       " '##sh': 8613,\n",
       " '便': 912,\n",
       " '##撒': 16111,\n",
       " '歹': 3646,\n",
       " '幟': 2392,\n",
       " '蓟': 5906,\n",
       " '佳': 881,\n",
       " '##忙': 15621,\n",
       " '##nba': 12360,\n",
       " '##烂': 17219,\n",
       " '鯨': 7809,\n",
       " '蟒': 6097,\n",
       " 'comhd': 12587,\n",
       " '##ヘ': 13693,\n",
       " '王': 4374,\n",
       " '##撤': 16116,\n",
       " '窗': 4970,\n",
       " '财': 6568,\n",
       " '嘮': 1668,\n",
       " '挞': 2910,\n",
       " '烟': 4170,\n",
       " '〗': 529,\n",
       " '##gg': 9949,\n",
       " '坊': 1773,\n",
       " '##侬': 13963,\n",
       " '濡': 4091,\n",
       " '桌': 3430,\n",
       " '匙': 1267,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 ms, sys: 615 µs, total: 29 ms\n",
      "Wall time: 28.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.2 ms, sys: 5.23 ms, total: 54.4 ms\n",
      "Wall time: 5.73 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 1.53 ms, total: 282 ms\n",
      "Wall time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 554 ms, sys: 1.51 ms, total: 556 ms\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 449 ms, sys: 8.85 ms, total: 458 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 466 ms, sys: 2.34 ms, total: 469 ms\n",
      "Wall time: 472 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2964\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2945\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2946\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2961\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2962\u001b[0m     )\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3029\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3030\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3035\u001b[0m )\n\u001b[0;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/tokenization_utils.py:711\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkyworkTokenizer(name_or_path='Skywork/Skywork-13B-base', vocab_size=65519, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新版本的transformers（>4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天宫的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-13B-base\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('skywork_tokenizer\\\\tokenizer_config.json',\n",
       " 'skywork_tokenizer\\\\special_tokens_map.json',\n",
       " 'skywork_tokenizer\\\\tokenizer.model',\n",
       " 'skywork_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"skywork_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skywork_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 dreaming! [SEP]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
